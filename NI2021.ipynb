{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwakjoonhyung/NLP/blob/main/NI2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVjYnT1l84oT",
        "outputId": "e3677ae7-b37c-46c1-9d02-73b422b2e958"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXq7VbS_8cdJ",
        "outputId": "db54d2d5-f5e2-401f-dad6-384b0fa88673"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "import sys"
      ],
      "metadata": {
        "id": "r8qgqLvIIi54"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.arrays import StringArray\n",
        "csv = pd.read_csv('datalist_2021.csv')\n",
        "abstract = csv['초록']\n",
        "abstract_values = []\n",
        "abstract_val = abstract.values\n",
        "for i in range(len(abstract_val)):  #nan 값 제외하기\n",
        "  if (abstract_val[i] == abstract_val[i]):\n",
        "    abstract_values.append(abstract_val[i])\n",
        "\n",
        "final_values = []\n",
        "for i in range(len(abstract_values)):\n",
        " final_values.append(re.sub(r'[0-9]',\"\",abstract_values[i])) #https://engineer-mole.tistory.com/238 (문자열에서 특정 문자만 삭제하는 방법)\n",
        "\n",
        "final_val = np.array(final_values)"
      ],
      "metadata": {
        "id": "4rjVRuV5QrKx"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_values"
      ],
      "metadata": {
        "id": "TjUPQuHLn0_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokenize=[]\n",
        "for i in abstract_val:\n",
        "  text_tokenize.append(sent_tokenize(str(i)))\n",
        "sentences=text_tokenize"
      ],
      "metadata": {
        "id": "cIXFxz2xQ_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "bigram = []\n",
        "stop_word = stopwords.words('english')\n",
        "f = open('eliminate_words.txt',\"r\")\n",
        "eliminate_words = f.read().splitlines()"
      ],
      "metadata": {
        "id": "Sy5zmWgyRFux"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_vec = CountVectorizer(stop_words=stop_word, ngram_range=(2,3))\n",
        "ngrams = c_vec.fit_transform(final_val.astype('U'))\n",
        "count_values = ngrams.toarray().sum(axis=0)\n",
        "vocab = c_vec.vocabulary_\n",
        "df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
        "            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "\n",
        "preprocessed_sentences = vocab.keys()"
      ],
      "metadata": {
        "id": "90xG3gA-RKI1"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences"
      ],
      "metadata": {
        "id": "ML8S3CI6zWng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for sentence in sentences:\n",
        " #tokenized_sentence = word_tokenize(str(sentence))\n",
        "#result = []\n",
        "\n",
        "#for i in range(len(sentences)):\n",
        "# for x in range(len(sentences[i])):\n",
        "#  bigram.append(list(ngrams(sentences[i][x].lower().split(),2)))\n",
        "\n",
        "#for word in tokenized_sentence:\n",
        "  #word = word.lower()\n",
        "  #if word not in stop_words:\n",
        "   #if word not in eliminate_words: \n",
        "    #result.append(word)\n",
        "    #if word not in vocab:\n",
        "      #vocab[word] = 0\n",
        "    #vocab[word] += 1\n",
        "#preprocessed_sentences.append(result)\n",
        "\n",
        "#sys.result = open('result_2018.txt','w')\n",
        "\n",
        "#vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
        "#print(vocab_sorted,file=sys.result)\n",
        "\n",
        "#sys.result.close()\n",
        "\n",
        "# word_to_index = {}\n",
        "# i = 0\n",
        "# for(word,frequency) in vocab_sorted:\n",
        "#  if frequency > 1 :\n",
        "#   i += 1\n",
        "#   word_to_index[word] = i\n",
        "\n",
        "# vocab_size = 7\n",
        "\n",
        "# words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# for w in words_frequency:\n",
        "#     del word_to_index[w]\n",
        "\n",
        "# word_to_index['OOV'] = len(word_to_index) + 1\n",
        "\n",
        "# encoded_sentences = []\n",
        "# for sentence in preprocessed_sentences:\n",
        "#     encoded_sentence = []\n",
        "#     for word in sentence:\n",
        "#         try:\n",
        "#             encoded_sentence.append(word_to_index[word])\n",
        "#         except KeyError:\n",
        "#             encoded_sentence.append(word_to_index['OOV'])\n",
        "#     encoded_sentences.append(encoded_sentence)\n",
        "# print(encoded_sentences)"
      ],
      "metadata": {
        "id": "vEq9DZVHP3g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "LVxQeeTPwTED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aKtgDiKx4ZK",
        "outputId": "9a8448fe-b644-45a8-be27-a525a99c601f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in encoded:\n",
        "  while len(sentence) < max_len:\n",
        "    sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "metadata": {
        "id": "8O8VqE8IA-Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlNIF5vKtFV",
        "outputId": "d631647e-79a3-4939-d93c-e17485c206b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in preprocessed_sentences:\n",
        "  if type(s)==int:\n",
        "    print(\"SDFSD\")"
      ],
      "metadata": {
        "id": "MvhpF0roJ30f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "숫자, 특수문자 불용어 데이터 전처리후 모델에 대입후 돌린다."
      ],
      "metadata": {
        "id": "Uz_nPV9-K9aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from gensim.models import Word2Vec\n",
        "#from gensim.models import KeyedVectors\n",
        "#model = Word2Vec(sentences=preprocessed_sentences, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "model = Phrases(sentences=preprocessed_sentences,min_count=5,threshold=7,progress_per=1000)"
      ],
      "metadata": {
        "id": "pC5d9sn_IR8q"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.save_word2vec_format('eng_phr2021')"
      ],
      "metadata": {
        "id": "SdZ5TsvBWZYx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "ceae09e0-51e5-4e79-a30e-97379d1a5e2e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-465345d54391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eng_phr2021'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Phrases' object has no attribute 'wv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m gensim.scripts.word2vec2tensor --input eng_phr2021 --output eng_phr2021"
      ],
      "metadata": {
        "id": "-TTArITNWoxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NI2021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM3bPKcZRq1jz/gt9kOzp65",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}