{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kwakjoonhyung/NLP/blob/main/NI2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVjYnT1l84oT",
        "outputId": "a69a3b9f-989c-4b41-f4da-da4d03f126ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 620 kB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXq7VbS_8cdJ",
        "outputId": "bc6ed6b4-4359-447d-8f90-841c23d5c1aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Okt\n",
        "import sys\n",
        "from pandas.core.arrays import StringArray"
      ],
      "metadata": {
        "id": "r8qgqLvIIi54"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv = pd.read_csv('datalist_2021.csv')\n",
        "abstract = csv['초록']\n",
        "abstract_values = []\n",
        "abstract_val = abstract.values\n",
        "for i in range(len(abstract_val)):  #nan 값 제외하기\n",
        "  if (abstract_val[i] == abstract_val[i]):\n",
        "    abstract_values.append(abstract_val[i])\n",
        "\n",
        "final_values = []\n",
        "for i in range(len(abstract_values)):\n",
        " final_values.append(re.sub(r'[0-9]',\"\",abstract_values[i])) #https://engineer-mole.tistory.com/238 (문자열에서 특정 문자만 삭제하는 방법)\n",
        "\n",
        "final_val = np.array(final_values)"
      ],
      "metadata": {
        "id": "4rjVRuV5QrKx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_val"
      ],
      "metadata": {
        "id": "TjUPQuHLn0_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "print(\"이차원 리스트 Sample : \")\n",
        "print(common_texts)\n"
      ],
      "metadata": {
        "id": "naIWl5c6ZNS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201eb33e-4e83-4958-d6c6-3820d0598e0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이차원 리스트 Sample : \n",
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokenize=[]\n",
        "for i in ``:\n",
        "  text_tokenize.append(sent_tokenize)\n",
        "sentences=text_tokenize"
      ],
      "metadata": {
        "id": "cIXFxz2xQ_yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}\n",
        "preprocessed_sentences = []\n",
        "bigram = []\n",
        "stop_word = stopwords.words('english')\n",
        "f = open('eliminate_words.txt',\"r\")\n",
        "eliminate_words = f.read().splitlines()"
      ],
      "metadata": {
        "id": "Sy5zmWgyRFux"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_vec = CountVectorizer(stop_words=stop_word, ngram_range=(2,3))\n",
        "ngrams = []\n",
        "for i in range(len(final_val)):\n",
        "  sent = []\n",
        "  c_vec.fit_transform([final_val[i].astype('U')]) #여기에서 c_vec에 ngram화된 단어들이 입력됨, U로 유니코드 문자열로 전환해줌\n",
        "  sent.append(c_vec.vocabulary_)\n",
        "  ngrams.append(sent)\n",
        "#count_values = ngrams.to_array().sum(axis=0)\n",
        "#ngrams_arr = np.array(ngrams)\n",
        "#count_values = ngrams_arr.sum(axis=0)\n",
        "#df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)\n",
        "#            ).rename(columns={0: 'frequency', 1:'bigram/trigram'})\n",
        "preprocessed_sentences = []\n",
        "for i in range(len(ngrams)):\n",
        "  keys = []\n",
        "  keys.append(ngrams[i][0].keys()) #뒤에 [0]을 붙이면 keys()가 읽힘 , 리스트에서 문자열로 바뀜\n",
        "  preprocessed_sentences.append(keys)"
      ],
      "metadata": {
        "id": "90xG3gA-RKI1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_sentences"
      ],
      "metadata": {
        "id": "ML8S3CI6zWng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for sentence in sentences:\n",
        " #tokenized_sentence = word_tokenize(str(sentence))\n",
        "#result = []\n",
        "\n",
        "#for i in range(len(sentences)):\n",
        "# for x in range(len(sentences[i])):\n",
        "#  bigram.append(list(ngrams(sentences[i][x].lower().split(),2)))\n",
        "\n",
        "#for word in tokenized_sentence:\n",
        "  #word = word.lower()\n",
        "  #if word not in stop_words:\n",
        "   #if word not in eliminate_words: \n",
        "    #result.append(word)\n",
        "    #if word not in vocab:\n",
        "      #vocab[word] = 0\n",
        "    #vocab[word] += 1\n",
        "#preprocessed_sentences.append(result)\n",
        "\n",
        "#sys.result = open('result_2018.txt','w')\n",
        "\n",
        "#vocab_sorted = sorted(vocab.items(), key = lambda x:x[1], reverse=True)\n",
        "#print(vocab_sorted,file=sys.result)\n",
        "\n",
        "#sys.result.close()\n",
        "\n",
        "# word_to_index = {}\n",
        "# i = 0\n",
        "# for(word,frequency) in vocab_sorted:\n",
        "#  if frequency > 1 :\n",
        "#   i += 1\n",
        "#   word_to_index[word] = i\n",
        "\n",
        "# vocab_size = 7\n",
        "\n",
        "# words_frequency = [word for word, index in word_to_index.items() if index >= vocab_size + 1]\n",
        "\n",
        "# for w in words_frequency:\n",
        "#     del word_to_index[w]\n",
        "\n",
        "# word_to_index['OOV'] = len(word_to_index) + 1\n",
        "\n",
        "# encoded_sentences = []\n",
        "# for sentence in preprocessed_sentences:\n",
        "#     encoded_sentence = []\n",
        "#     for word in sentence:\n",
        "#         try:\n",
        "#             encoded_sentence.append(word_to_index[word])\n",
        "#         except KeyError:\n",
        "#             encoded_sentence.append(word_to_index['OOV'])\n",
        "#     encoded_sentences.append(encoded_sentence)\n",
        "# print(encoded_sentences)"
      ],
      "metadata": {
        "id": "vEq9DZVHP3g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_sentences)\n",
        "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "id": "LVxQeeTPwTED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print('최대 길이 :',max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aKtgDiKx4ZK",
        "outputId": "9a8448fe-b644-45a8-be27-a525a99c601f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최대 길이 : 113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in encoded:\n",
        "  while len(sentence) < max_len:\n",
        "    sentence.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ],
      "metadata": {
        "id": "8O8VqE8IA-Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlNIF5vKtFV",
        "outputId": "d631647e-79a3-4939-d93c-e17485c206b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in preprocessed_sentences:\n",
        "  if type(s)==int:\n",
        "    print(\"SDFSD\")"
      ],
      "metadata": {
        "id": "MvhpF0roJ30f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from gensim.models import Word2Vec\n",
        "#from gensim.models import KeyedVectors\n",
        "#model = Word2Vec(sentences=preprocessed_sentences, size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "model = Phrases(sentences=preprocessed_sentences,min_count=5,threshold=7,progress_per=1000)\n"
      ],
      "metadata": {
        "id": "pC5d9sn_IR8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('eng_phr2021', separately=None, sep_limit=10485760, ignore=frozenset([]), pickle_protocol=2)"
      ],
      "metadata": {
        "id": "SdZ5TsvBWZYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m gensim.scripts.word2vec2tensor --input eng_phr2021 --output eng_phr2021"
      ],
      "metadata": {
        "id": "-TTArITNWoxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwpOJRgo5SV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNtHJnYf+BFN57YFTD/T6jk",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}